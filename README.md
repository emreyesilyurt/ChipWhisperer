# ChipWhisperer PDF Extractor (GPT-4V + Llama 3)

## ğŸ“Œ Overview
ChipWhisperer is a **hybrid PDF text extraction tool** that intelligently determines whether a PDF is **image-based or text-based** and applies the most suitable **Large Language Model (LLM)** to extract meaningful content.

### ğŸ”¹ **How it Works**
- If the PDF **contains selectable text**, the tool **uses Llama 3** (via Ollama) to extract and summarize the text.
- If the PDF is **scanned (image-based)**, the tool **uses GPT-4V** (GPT-4 Vision) to **intelligently extract** the text.

This approach ensures **higher accuracy** compared to traditional OCR while leveraging **LLM capabilities** to improve text structuring.

---

## ğŸš€ Features
âœ… **Automated PDF Classification** - Detects whether a PDF is text-based or image-based.  
âœ… **GPT-4V for Image-Based PDFs** - Extracts text from scanned or image-based documents.  
âœ… **Llama 3 for Text-Based PDFs** - Extracts and summarizes content from PDFs with selectable text.  
âœ… **Modular Structure** - Supports easy modifications and enhancements.  
âœ… **Configurable Processing** - Uses a configuration file to manage LLM model selections and parameters.  
âœ… **Logging & Pre/Post Processing** - Keeps logs and allows additional text cleanup.  
âœ… **Command-line Interface** - Easy to use from the command line with various options.

---

## ğŸ— Project Structure
```
ChipWhisperer/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ main.py                        # Entry point with CLI interface
â”‚   â”œâ”€â”€ vllm_pdf_extractor.py          # Core extraction logic (GPT-4V & Llama 3)
â”‚â”€â”€ utils/
â”‚   â”œâ”€â”€ helper.py                      # General helper functions
â”‚   â”œâ”€â”€ logger.py                      # Logging utilities
â”‚   â”œâ”€â”€ pdf_reader.py                  # PDF classification and metadata
â”‚â”€â”€ classifier/
â”‚   â”œâ”€â”€ model.py                       # Placeholder for future ML-based classifiers
â”‚   â”œâ”€â”€ preprocessor.py                # Preprocessing for PDFs
â”‚   â”œâ”€â”€ postprocessor.py               # Post-processing extracted text
â”‚â”€â”€ tests/
â”‚   â”œâ”€â”€ test_vllm_pdf_extractor.py     # Unit tests for extraction logic
â”‚   â”œâ”€â”€ test_pdf_reader.py             # Unit tests for PDF classification
â”‚â”€â”€ configs/
â”‚   â”œâ”€â”€ config.json                    # Configuration settings
â”‚â”€â”€ logs/                              # Directory for log files
â”‚â”€â”€ examples/
â”‚   â”œâ”€â”€ sample.pdf                     # Example PDFs
â”‚â”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ README.md                          # Project documentation
```

---

## ğŸ›  Installation

### **ğŸ”¹ Prerequisites**
Ensure you have:
- **Python 3.8+**
- **Ollama installed** (for Llama 3) - https://ollama.com/
- **OpenAI API Key** (for GPT-4V access)

### **ğŸ”¹ Install Dependencies**
Clone the repository and install the required dependencies:

```bash
git clone https://github.com/emreyesilyurt/chipwhisperer.git
cd chipwhisperer
pip install -r requirements.txt
```

### **ğŸ”¹ Set Up Environment Variables**
```bash
# Set your OpenAI API key (required for GPT-4V)
export OPENAI_API_KEY=your_openai_api_key

# Make sure Ollama is running (for Llama 3)
# Default URL: http://localhost:11434
```

---

## ğŸš€ Usage

### **ğŸ”¹ Basic Usage**
To process a PDF with automatic detection:

```bash
python src/main.py --pdf path/to/your/document.pdf
```

### **ğŸ”¹ Command-line Options**
```bash
# Process a PDF and save the output to a file
python src/main.py --pdf input.pdf --output results.txt

# Force using GPT-4V regardless of PDF type
python src/main.py --pdf input.pdf --force-vllm

# Force using Llama 3 regardless of PDF type
python src/main.py --pdf input.pdf --force-llama
```

### **ğŸ”¹ Running Tests**
Run the unit tests:

```bash
python -m unittest discover tests
```

---

## âš™ï¸ Configuration

You can customize the tool's behavior by modifying `configs/config.json`:

```json
{
    "vllm_model": "gpt-4-vision-preview",
    "llama3_model": "llama3",
    "extraction": {
        "image_quality": 2,
        "max_tokens": 4096,
        "chunk_size": 32000,
        "temperature": 0.0
    },
    "classification": {
        "text_threshold": 50,
        "text_page_ratio": 0.5
    },
    "ollama": {
        "api_url": "http://localhost:11434/api/chat"
    },
    "processing": {
        "clean_text": true,
        "extract_sections": false,
        "section_headings": [
            "Abstract",
            "Introduction",
            "Background",
            "Methods",
            "Results",
            "Discussion",
            "Conclusion",
            "References"
        ]
    }
}
```

---

## ğŸ¯ Future Improvements
- **Fine-tuned models** for specific document types
- **Layout recognition** for complex PDF structures (tables, multi-columns)
- **Enhanced section detection** for academic papers
- **Batch processing** for handling multiple PDFs
- **API interface** for integration with other applications
- **Web interface** for easier use

---

## ğŸ“œ License
MIT License - See LICENSE file for details.

## ğŸ”— Contributing
Contributions are welcome! Feel free to open issues or submit pull requests.
